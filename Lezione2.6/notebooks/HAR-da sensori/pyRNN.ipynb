{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pyRNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QpMq7glTZO-4"},"source":["##LSTMs for Human Activity Recognition Time Series Classification\n","\n","Paper:  https://upcommons.upc.edu/bitstream/handle/2117/101769/IWAAL2012.pdf?sequence=1\n"]},{"cell_type":"code","metadata":{"id":"TeiDDKN1Xbxb"},"source":["from numpy import mean\n","from numpy import std\n","from numpy import dstack\n","from pandas import read_csv\n","import tensorflow as tf  \n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.utils import to_categorical\n","from matplotlib import pyplot "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTKfsT80VBS8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmHrxvEP54xu"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive\n","\n","# path of Drive folder\n","data_dir = '/gdrive/My Drive/Informazione_Multimediale/Colab_Notebooks/video/datasets/UCI_HAR/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6r3UpupuWPOt"},"source":["# Load data\n","\n","There are 3 main signal types in the raw data: \n","1. total acceleration\n","2. body acceleration\n","3. body gyroscope\n","\n","Each has **3 axes** of data. This means that there are a total of **9 variables** for each time step.\n","\n","Further, each series of data has been partitioned into **overlapping windows of 2.56 seconds** of data, or **128 time steps**. These windows of data correspond to the windows of engineered features (rows). This means that one row of data has  **128 * 9 = 1152** elements.\n","\n","The signals are stored in the */Inertial Signals/* directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions.\n","\n","The input data is in CSV format where columns are separated by whitespace. Each of these files can be loaded as a NumPy array. The *load_file()* function below loads a dataset given the fill path to the file and returns the loaded data as a NumPy array.\n"]},{"cell_type":"code","metadata":{"id":"8qyHoQjIY5Fh"},"source":["# load a single file as a numpy array\n","def load_file(filepath):\n","\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n","\treturn dataframe.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnNANx7IY5z8"},"source":["We can then load all data for a given group (train or test) into a single three-dimensional NumPy array, where the dimensions of the array are **[samples, time steps, features]**.\n","\n","To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file.\n","\n","The *load_group()* function below implements this behavior. The *dstack()* NumPy function allows us to stack each of the loaded 3D arrays into a single 3D array where the variables are separated on the third dimension (features).\n","\n"]},{"cell_type":"code","metadata":{"id":"Zky7ZBh4awqf"},"source":["# load a list of files and return as a 3d numpy array\n","def load_group(filenames, prefix=''):\n","\tloaded = list()\n","\tfor name in filenames:\n","\t\tdata = load_file(prefix + name)\n","\t\tloaded.append(data)\n","\t# stack group so that features are the 3rd dimension\n","\tloaded = dstack(loaded)\n","\treturn loaded"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KlL6OqKaxQ9"},"source":["##Load all input signal data for a given group, such as train or test.\n","\n","The load_dataset_group() function below loads all input signal data and the output data for a single group using the consistent naming conventions between the directories."]},{"cell_type":"code","metadata":{"id":"aRk542ACcXAI"},"source":["# load a dataset train or test group\n","def load_dataset_group(path, group):\n","\t# load all 9 files as a single array\n","  filenames = list()\n","  filepath = path + group + '/Inertial Signals/'\n","\n","  # total acceleration\n","  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n","  # body acceleration\n","  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n","  # body gyroscope\n","  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n","\n","  # load input data\n","  X = load_group(filenames, filepath)\n","  # load class output\n","  y = load_file(path + group + '/y_'+group+'.txt')\n","  \n","  return X, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z2yXpWRsc27q"},"source":["The output data is defined as an integer for the class number. We must one hot encode these class integers so that the data is suitable for fitting a neural network multi-class classification model. We can do this by calling the to_categorical() Keras function.\n","\n","The load_dataset() function below implements this behavior and returns the train and test X and y elements ready for fitting and evaluating the defined models."]},{"cell_type":"code","metadata":{"id":"eHK1iTQLc8cr"},"source":["# load the dataset, returns train and test X and y elements\n","def load_dataset(path):\n","  # load all train\n","\n","  trainX, trainy = load_dataset_group(path, 'train')\n","  print('Train X dim : ', trainX.shape)\n","  print('Train y dim : ', trainy.shape)\n","\n","  # load all test\n","  testX, testy = load_dataset_group(path, 'test')\n","  print('Test X dim  : ', testX.shape)\n","  print('Test y dim  : ', testy.shape)\n","\n","  # zero-offset class values\n","  trainy = trainy - 1\n","  testy = testy - 1\n","\n","  # one hot encode y\n","  trainy = to_categorical(trainy)\n","  testy = to_categorical(testy)\n","  #print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n","  return trainX, trainy, testX, testy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZo3-EqkdDL_"},"source":["# Fit and Evaluate Model\n","\n","First, we must define the LSTM model using the Keras deep learning library. The model requires a three-dimensional input with **[samples, time steps, features]**.\n","\n","This is exactly how we have loaded the data, where one sample is one window of the time series data, each window has 128 time steps, and a time step has nine variables or features.\n","\n","The output for the model will be a six-element vector containing the probability of a given window belonging to each of the six activity types.\n","\n","Thees input and output dimensions are required when fitting the model, and we can extract them from the provided training dataset.\n","```\n","n_timesteps = trainX.shape[1]\n","n_features = trainX.shape[2]\n","n_outputs = trainy.shape[1]\n","```\n","The model is defined as a **Sequential Keras model**:\n","\n","* the model has a **single** LSTM **hidden layer**\n","* a **dropout layer** (to reduce overfitting)\n","* a **dense fully connected hidden layer** (features extracted by the LSTM)\n","* final output layer is used to make predictions\n","```\n","model = Sequential()\n","model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n","model.add(Dropout(0.5))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(n_outputs, activation='softmax'))\n","```"]},{"cell_type":"markdown","metadata":{"id":"4fcJ_I69j-mN"},"source":["The model is fit for a **fixed number of epochs**, in **this case 15**, and a **batch size of 64 samples** will be used, where 64 windows of data will be exposed to the model before the weights of the model are updated.\n","\n","Once the model is fit, it is evaluated on the test dataset and the accuracy of the fit model on the test dataset is returned.\n","\n","Note, it is common to not shuffle sequence data when fitting an LSTM. Here we do shuffle the windows of input data during training (the default). In this problem, we are interested in harnessing the LSTMs ability to learn and extract features across the time steps in a window, not across windows.\n","\n","The complete *evaluate_model()* function is listed below."]},{"cell_type":"code","metadata":{"id":"oLhy10rC16bH"},"source":["# lstm model: fit and evaluate a model\n","def evaluate_model(trainX, trainy, testX, testy):\n","\tverbose, epochs, batch_size = 0, 15, 64\n","\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n","\tmodel = Sequential()\n","\tmodel.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n","\tmodel.add(Dropout(0.5))\n","\tmodel.add(Dense(100, activation='relu'))\n","\tmodel.add(Dense(n_outputs, activation='softmax'))\n"," \n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# fit network\n","\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","\t# evaluate model\n","\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n","\treturn accuracy, model\n","\n","# summarize scores\n","def summarize_results(scores):\n","\tprint(scores)\n","\tm, s = mean(scores), std(scores)\n","\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKh1pzyii023"},"source":["\n","The efficient Adam version of stochastic gradient descent may be used to optimize the network, and the categorical cross entropy loss function may be used given that we are learning a multi-class classification problem. Alternatively, it is possible to choose among the following"]},{"cell_type":"code","metadata":{"id":"lT-halIlpxBN"},"source":["# Optimizers https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","sgd = tf.keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n","adad = tf.keras.optimizers.Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\n","adag = tf.keras.optimizers.Adagrad(lr=0.01,epsilon=None,decay=0.0)\n","adamax = tf.keras.optimizers.Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\n","nadam = tf.keras.optimizers.Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\n","rms = tf.keras.optimizers.RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n","\n","# Losses https://keras.io/losses/\n","loss = ['sparse_categorical_crossentropy','binary_crossentropy','mean_squared_error','mean_absolute_error',\n","        'categorical_crossentropy','categorical_hinge']\n","\n","# Metrics  https://www.tensorflow.org/api_docs/python/tf/metrics\n","metrics = ['accuracy','precision','recall']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCHeYUX2BQyb"},"source":["# run an experiment\n","repeats = 5\n","# load data\n","trainX, trainy, testX, testy = load_dataset(data_dir)\n","\n","# repeat experiment\n","scores = list()\n","for r in range(repeats):\n","  score, model = evaluate_model(trainX, trainy, testX, testy)\n","  score = score * 100.0\n","  print('Accuracy at round #%d: %.3f' % (r+1, score))\n","  scores.append(score)\n","\n","# summarize results\n","m, s = mean(scores), std(scores)\n","print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n","model.summary()"],"execution_count":null,"outputs":[]}]}